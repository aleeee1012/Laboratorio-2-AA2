{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec185c69",
   "metadata": {},
   "source": [
    "# Laboratorio 2\n",
    "## Pipeline de Machine Learning para Clasificación de Sentimientos con Transformers\n",
    "\n",
    "En este laboratorio, realizarás una clasificación de sentimiento de reseñas de películas mediante el modelo de transformadores. Como parte de este laboratorio, implementarás un análisis exploratorio de datos para obtener conocimientos sobre la data y su distribución. Posteriormente, los modelos requieren un tokenizer que implementarás personalmente. A continuación, programarás el elemento integral del modelo de transformador, la Mecánica de Atención. Finalmente, combinarás todo y entrenarás al modelo en la tarea de clasificación binaria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3de077",
   "metadata": {},
   "source": [
    "## Tutorial: Cómo activar la GPU gratuita en Google Colab\n",
    "\n",
    "Para este laboratorio, se recomienda usar la GPU gratuita de Google Colab para acelerar el entrenamiento. Sigue estos pasos:\n",
    "\n",
    "### Pasos para activar la GPU:\n",
    "\n",
    "1. **Abre tu notebook en Google Colab**\n",
    "   - Sube este archivo a Google Drive\n",
    "   - Haz clic derecho sobre el archivo → \"Abrir con\" → \"Google Colaboratory\"\n",
    "\n",
    "2. **Cambia el tipo de entorno de ejecución**\n",
    "   - En el menú superior, ve a: **Entorno de ejecución** → **Cambiar tipo de entorno de ejecución**\n",
    "   - (O en inglés: **Runtime** → **Change runtime type**)\n",
    "\n",
    "3. **Selecciona la GPU**\n",
    "   - En \"Acelerador por hardware\" (Hardware accelerator), selecciona T4 GPU\n",
    "   - Haz clic en **Guardar** (Save)\n",
    "\n",
    "4. **Verifica que la GPU esté activa**\n",
    "   - Ejecuta la siguiente celda para confirmar que tienes acceso a la GPU\n",
    "\n",
    "### Notas importantes:\n",
    "- La GPU gratuita tiene límites de uso diario (aproximadamente 12-15 horas por día)\n",
    "- Si se desconecta, deberás volver a ejecutar todas las celdas\n",
    "- Guarda tu progreso frecuentemente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6087f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que la GPU está disponible\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU detectada: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memoria total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"✗ No se detectó GPU. Por favor, activa la GPU siguiendo los pasos del tutorial anterior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6314a9fe",
   "metadata": {},
   "source": [
    "## Instalación de Dependencias\n",
    "\n",
    "Primero, instalaremos las bibliotecas necesarias para este laboratorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac61acfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb46f3",
   "metadata": {},
   "source": [
    "## Imports y Setup Inicial\n",
    "\n",
    "Importamos todas las bibliotecas necesarias y configuramos las semillas aleatorias para reproducibilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b7a2fa",
   "metadata": {},
   "source": [
    "## Análisis Exploratorio de Datos (EDA)\n",
    "\n",
    "### 1.- Carga de Datos\n",
    "\n",
    "Cargaremos el dataset IMDB que contiene reseñas de películas con sentimientos positivos y negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ababe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "texts = dataset['train']['text'] + dataset['test']['text']\n",
    "labels = dataset['train']['label'] + dataset['test']['label']\n",
    "\n",
    "print(f\"Total de textos: {len(texts)}\")\n",
    "print(f\"Etiquetas únicas: {np.unique(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956bac9",
   "metadata": {},
   "source": [
    "### 2.- Visualización de Distribuciones\n",
    "\n",
    "Analizaremos la distribución de longitudes de las reseñas y la distribución de las etiquetas (positivo/negativo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "review_lengths = [word_count(text) for text in texts]\n",
    "\n",
    "# Plot the distribution of review lengths\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(review_lengths, bins=30, kde=True)\n",
    "plt.title('Gráfico de distribución de longitudes de reseñas')\n",
    "plt.xlabel('Cantidad de palabras')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n",
    "\n",
    "# Plot the distribution of labels\n",
    "label_counts = Counter(labels)\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.barplot(x=list(label_counts.keys()), y=list(label_counts.values()))\n",
    "plt.title('Gráfico de distribución de etiquetas')\n",
    "plt.xlabel('Etiqueta')\n",
    "plt.ylabel('Conteo')\n",
    "plt.xticks(ticks=[0, 1], labels=['Negativo', 'Positivo'])\n",
    "plt.show()\n",
    "\n",
    "average_positive_length = np.mean([review_lengths[i] for i in range(len(labels)) if labels[i] == 1])\n",
    "average_negative_length = np.mean([review_lengths[i] for i in range(len(labels)) if labels[i] == 0])\n",
    "\n",
    "print(f\"Promedio de longitud de reseñas positivas: {average_positive_length:.2f} palabras\")\n",
    "print(f\"Promedio de longitud de reseñas negativas: {average_negative_length:.2f} palabras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c84fe",
   "metadata": {},
   "source": [
    "## Tareas\n",
    "\n",
    "### 1: Implementación del Tokenizador\n",
    "\n",
    "- [ ] Ampliar el tokenizador para que construya un vocabulario de 30.000 palabras y fragmentos de palabra (libro capítulo 12.5).\n",
    "- [ ] Mostrar el tamaño del vocabulario a lo largo de las iteraciones del algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer Implementation\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el tokenizador con las variables necesarias para construir el vocabulario.\n",
    "        El vocabulario contiene los tokens especiales y su código, mientras que la reversa_vocabulario\n",
    "        contiene el código de cada token como clave y el token mismo como valor.\n",
    "        \"\"\"\n",
    "        self.special_tokens = {'[PAD]': 0, '[UNK]': 1} # Tokens especiales con sus códigos\n",
    "        self.vocab = {'[PAD]': 0, '[UNK]': 1} # Vocabulario inicializado con tokens especiales\n",
    "        self.reverse_vocab = {0: '[PAD]', 1: '[UNK]'} # Reversa vocabulario inicializado\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Convierte el texto a una lista de tokens al convertirlo a minúscula y separarla por espacios.\n",
    "        \"\"\"\n",
    "        tokens = text.lower().split()\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"\n",
    "        Crea el vocabulario a partir de la lista de textos proporcionados.\n",
    "        Se agrega cada token nuevo al vocabulario y se actualiza la reversa_vocabulario correspondiente.\n",
    "        \"\"\"\n",
    "        # TODO Ampliar el tokenizador para que construya un vocabulario de 30.000 palabras y fragmentos de palabra (libro capítulo 12.5).\n",
    "        index = len(self.vocab)\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocab:\n",
    "                    self.vocab[token] = index\n",
    "                    self.reverse_vocab[index] = token\n",
    "                    index += 1\n",
    "\n",
    "    def encode(self, text, max_length=128):\n",
    "        \"\"\"\n",
    "        Convierte el texto a una lista de IDs de tokens utilizando el vocabulario.\n",
    "        Si la longitud de los IDs es menor que la máxima longitud, se agregan [PAD] repetidos para alcanzar la longitud deseada.\n",
    "        \"\"\"\n",
    "        # TODO Ampliar el tokenizador para que construya un vocabulario de 30.000 palabras y fragmentos de palabra (libro capítulo 12.5).\n",
    "        tokens = self.tokenize(text)\n",
    "        token_ids = [self.vocab.get(token, self.vocab['[UNK]']) for token in tokens]\n",
    "        if len(token_ids) < max_length:\n",
    "            token_ids += [self.vocab['[PAD]']] * (max_length - len(token_ids))\n",
    "        else:\n",
    "            token_ids = token_ids[:max_length]\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Convierte la lista de IDs de tokens a un texto utilizando la reversa_vocabulario.\n",
    "        \"\"\"\n",
    "        return ' '.join(self.reverse_vocab.get(id, '[UNK]') for id in token_ids)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "simple_tokenizer = SimpleTokenizer()\n",
    "simple_tokenizer.build_vocab(texts)\n",
    "print(f\"Tamaño del vocabulario: {simple_tokenizer.vocab_size}\")\n",
    "if simple_tokenizer.vocab_size >= 29900 or simple_tokenizer.vocab_size <= 30100:\n",
    "    print(f\"Vocabulary is incorrect: {simple_tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d377dc0",
   "metadata": {},
   "source": [
    "### Prueba del Tokenizador\n",
    "\n",
    "Verificaremos que el tokenizador funciona correctamente codificando y decodificando un texto de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae542e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ejemplo de codificación:\")\n",
    "print(simple_tokenizer.encode(\"The movie was maravilloso\"))\n",
    "print(\"\\nEjemplo de decodificación:\")\n",
    "print(simple_tokenizer.decode(simple_tokenizer.encode(\"The movie was maravilloso\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20128a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Imprimir 15 de la totalidad del vocabulario de tamaño {simple_tokenizer.vocab_size}\")\n",
    "for idx, key in enumerate(simple_tokenizer.vocab):\n",
    "    print(f\"palabra o fragmento correspondiente '{key}' con su token {simple_tokenizer.vocab[key]}\")\n",
    "    if idx == 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03fece1",
   "metadata": {},
   "source": [
    "## Preparación del Conjunto de Datos\n",
    "\n",
    "Este conjunto de datos se ha reducido a 1500 desde 50.000 para que puedas desarrollar en una instancia de **Google Colab con GPU**. Con la GPU activada, el entrenamiento será mucho más rápido. Si usas CPU, también será posible pero tomará más tiempo.\n",
    "\n",
    "Crearemos los conjuntos de entrenamiento, validación y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e27a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación del conjunto de datos IMDB\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, tokenizer, max_length=128, seed=42):\n",
    "        dataset = load_dataset('imdb')\n",
    "        texts, labels = dataset['train']['text'] + dataset['test']['text'], dataset['train']['label'] + dataset['test']['label']\n",
    "        data = list(zip(texts, labels))\n",
    "        random.Random(seed).shuffle(data)\n",
    "        self.texts, self.labels = zip(*data)\n",
    "        self.texts = self.texts[:1500]\n",
    "        self.labels = self.labels[:1500]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer.encode(text, max_length=self.max_length)\n",
    "        return torch.tensor(inputs, dtype=torch.long), label\n",
    "\n",
    "# longitud de secuencia\n",
    "max_length = 128\n",
    "dataset = IMDBDataset(simple_tokenizer, max_length=max_length)\n",
    "\n",
    "# Dividir el conjunto de datos en conjuntos de entrenamiento, validación y prueba\n",
    "train_size = 800\n",
    "val_size = 200\n",
    "test_size = 500\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(random_seed))\n",
    "\n",
    "# DataLoader para batchear\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(f\"Tamaños de los conjuntos - Entrenamiento: {len(train_dataset)}, Validación: {len(val_dataset)}, Prueba: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956d7d0",
   "metadata": {},
   "source": [
    "## Modelo\n",
    "\n",
    "### 2: Mecanismo de Atención\n",
    "\n",
    "- [ ] Capa de atención unidireccional **SingleHeadAttention** con matrices de Query, Key y Value $Q_q, Q_k, Q_v$ (libro capítulo 12.2)\n",
    "- [ ] **Ajustado** Punto de dot producto de escala (libro capítulo 12.3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a1d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementación de la atención unidireccional escalada por punto de escala, \n",
    "    utilizada en las transformadoras.\n",
    "    \n",
    "    - Utiliza la función dot producto escalada para calcular los pesos de atención.\n",
    "    - Aplica la función softmax para obtener las probabilidades de atención.\n",
    "    - Multiplica las probabilidades de atención con el valor correspondiente para obtener el output.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        \"\"\"\n",
    "        Procesar los inputs y obtener el output.\n",
    "        \n",
    "        - Calcula las matrices de dot producto entre query y key.\n",
    "        - Divide las matrices de dot producto por la escala.\n",
    "        - Aplica la función softmax para obtener las probabilidades de atención.\n",
    "        - Multiplica las probabilidades de atención con el valor correspondiente para obtener el output.\n",
    "        \n",
    "        Returns:\n",
    "            output (tensor): El output de la capa de atención.\n",
    "            attn_weights (tensor): Las probabilidades de atención.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        return output, attn_weights\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementación de la atención unidireccional con varias capas lineales, \n",
    "    utilizada en las transformadoras.\n",
    "    \n",
    "    - Utiliza varias capas lineales para calcular las matrices de query, key y value.\n",
    "    - Aplica la capa de atención escalada por punto de escala para obtener las probabilidades de atención.\n",
    "    - Multiplica las probabilidades de atención con el valor correspondiente para obtener el output final.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model):\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Procesar los inputs y obtener el output.\n",
    "        \n",
    "        - Calcula las matrices de query, key y value utilizando varias capas lineales.\n",
    "        - Aplica la capa de atención escalada por punto de escala para obtener las probabilidades de atención.\n",
    "        - Multiplica las probabilidades de atención con el valor correspondiente para obtener el output final.\n",
    "        \n",
    "        Returns:\n",
    "            output (tensor): El output final de la capa de atención.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        output = 0\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02624c99",
   "metadata": {},
   "source": [
    "### 3: Capa del Transformer\n",
    "\n",
    "- [ ] Unir los elementos de la capa SingleHeadAttention, Block de normalización por capa (nn.LayerNorm) y el bloque de forward feedforward (nn.Linear) con una capa de normalización final (libro capítulo 12.4)\n",
    "- [ ] No olvidar las conexiones residuales\n",
    "\n",
    "En general, se recomienda utilizar ReLU y además técnicas de regularización para mejorar la generalización del modelo a los datos de prueba (como Dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed45d7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Una capa única en la arquitectura del codificador de transformer.\n",
    "    \n",
    "    Esta clase define un paso adelante que aplica atención autoregresa, seguido de dos capas lineales y normalización de capa.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model, dim_feedforward):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Paso adelante a través de la capa del codificador de transformer.\n",
    "        \n",
    "        Parámetros:\n",
    "            src (Tensor): Tensor de entrada a ser procesado por la capa, torch.Size([8, 128, 32])\n",
    "        \n",
    "        Regresa:\n",
    "            Tensor: Tensor de salida después de aplicar la atención autoregresa y el FFN.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d36dd0",
   "metadata": {},
   "source": [
    "### Arquitectura del Modelo\n",
    "\n",
    "El **SentimentAnalysisModel** básico se proporciona. Este modelo combina todos los componentes anteriores para crear un clasificador de sentimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64271fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super(SentimentAnalysisModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder = TransformerEncoderLayer(embed_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src)\n",
    "        encoded_output = self.encoder(src)\n",
    "        pooled_output = encoded_output.mean(dim=1)\n",
    "        return self.fc(pooled_output)\n",
    "\n",
    "# Puedes experimentar con diferentes parámetros hiperbásicos, como la embedding o el dimensión oculta.\n",
    "# Tal como puedes ajustar los parámetros de aprendizaje para mejorar el rendimiento del modelo.\n",
    "embed_dim = 32\n",
    "hidden_dim = 128\n",
    "num_classes = 2\n",
    "vocab_size = simple_tokenizer.vocab_size\n",
    "model = SentimentAnalysisModel(vocab_size, embed_dim, hidden_dim, num_classes)\n",
    "\n",
    "# Test with synthetic data\n",
    "fake_data = torch.randint(0, vocab_size, (8, max_length))\n",
    "fake_output = model(fake_data)\n",
    "assert fake_output.shape == (8, num_classes), \"Output shape is incorrect\"\n",
    "print(f\"Modelo creado exitosamente. Shape de salida: {fake_output.shape}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb24eb",
   "metadata": {},
   "source": [
    "## Fase de Entrenamiento\n",
    "\n",
    "### 4: Entrenamiento\n",
    "\n",
    "- [ ] Implementar el mecanismo de detección temprana o otra monitoreo del progreso de aprendizaje\n",
    "- [ ] Experimentar con tamaños más grandes de modelos y comparar el rendimiento obtenido con la cantidad de parámetros requeridos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e1db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Bucle de entrenamiento con validación\n",
    "for epoch in range(1): # TODO aumentar el número de épocas para entrenar modelos más potentes\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for texts, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(texts)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch: {epoch+1}, Training Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Bucle de validación\n",
    "    # TODO Implementar el mecanismo de detección temprana o otra monitoreo del progreso de aprendizaje\n",
    "    model.eval()\n",
    "    val_correct, val_total = 0, 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            outputs = model(texts)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f'Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9fc40",
   "metadata": {},
   "source": [
    "## Análisis de Resultados\n",
    "\n",
    "### 5: Evaluación en el Conjunto de Test\n",
    "\n",
    "- [ ] Comparar el rendimiento del modelo con muestras de prueba de diferentes longitudes\n",
    "- [ ] Bonus: alcanzar una precisión superior al 90%. Es posible que debas incrementar el numero de parámetros y que debas implementar la atención múltiple AutoAtención y multicapa para lograr esto (libro capítulo 12.3.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad93eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Test loop\n",
    "model.eval()\n",
    "test_correct, test_total = 0, 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        outputs = model(texts)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.append(predicted[0])\n",
    "        y_true.append(labels[0])\n",
    "        \n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "print(\"\\nMatriz de confusión:\")\n",
    "print(confusion_matrix(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114065bb",
   "metadata": {},
   "source": [
    "## Despliegue\n",
    "\n",
    "### 6: Pruebas con Ejemplos Personalizados\n",
    "\n",
    "- [ ] Intenta crear nuevos ejemplos donde el modelo falla al clasificar correctamente el sentimiento.\n",
    "- [ ] Experimenta, describe y explica qué ocurre cuando intentas clasificar una crítica de cine en español utilizando tu modelo de transformador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e588937",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"The movie was amazing\"\n",
    "inputs = simple_tokenizer.encode(review, max_length=max_length)\n",
    "inputs = torch.tensor([inputs], dtype=torch.long)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "    _, predicted_class = torch.max(outputs, 1)\n",
    "\n",
    "class_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "predicted_label = class_labels[predicted_class.item()]\n",
    "print(f\"The predicted class for the text '{review}' is: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f7a7b",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Analiza los resultados obtenidos:\n",
    "\n",
    "- ¿Qué tipo de reseñas fueron más difíciles de clasificar para el modelo? ¿A qué crees que se debe?\n",
    "- ¿Qué mejoras podrías proponer para el modelo o el proceso de entrenamiento?\n",
    "- ¿Qué sucede cuando pruebas el modelo con texto en español?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
